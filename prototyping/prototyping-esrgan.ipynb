{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import psutil\n",
    "from util import format\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datasets import DeepLenseSuperresolutionDataset\n",
    "import matplotlib.pyplot as plt \n",
    "from torchinfo import summary\n",
    "from util import run_experiment_task2, MSE_Metric, PSNR_Metric, SSIM_Metric\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class CONFIG:\n",
    "    BATCH_SIZE = 4\n",
    "    EPOCHS = 10\n",
    "    COMPILE = False\n",
    "    PRINT_FREQ = 100\n",
    "    SCALE=2\n",
    "    \n",
    "    # limit the data to prototype faster\n",
    "    DATA_LIMIT = 29\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    ROUND_NUMBER = 3\n",
    "    TASK_NAME = \"DeepLense2024_task2A\"\n",
    "    DATA_PATH = os.path.join(\"Data\", \"Superresolution\")\n",
    "    PORTION_OF_DATA_FOR_TRAINING = 0.8\n",
    "    \n",
    "    PRETRAINED_G_MODEL = False\n",
    "    PRETRAINED_D_MODEL = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_start = psutil.virtual_memory()\n",
    "print(f\"RAM used : {format(memory_start.used)}\")\n",
    "\n",
    "dataset = DeepLenseSuperresolutionDataset(folder_path=CONFIG.DATA_PATH, \n",
    "                                          randomize_dataset=True,\n",
    "                                          data_limit=CONFIG.DATA_LIMIT)\n",
    "\n",
    "# do train/val split\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * CONFIG.PORTION_OF_DATA_FOR_TRAINING)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "print(f\"splitting into : {train_size} {val_size}\")\n",
    "\n",
    "# splitting dataset \n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"sizes of datasets : len(train)={len(train_dataset)} len(val)={len(val_dataset)}\")\n",
    "\n",
    "# Create Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample batch, useful for prototyping the architectures\n",
    "x_, y_ = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Dakewe Biotech Corporation. All Rights Reserved.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#   you may not use this file except in compliance with the License.\n",
    "#   You may obtain a copy of the License at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "import os\n",
    "from typing import Any, cast, Dict, List, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F_torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "__all__ = [\n",
    "    \"DiscriminatorForVGG\", \"RRDBNet\", \"ContentLoss\",\n",
    "    \"discriminator_for_vgg\", \"rrdbnet_x2\", \"rrdbnet_x4\", \"rrdbnet_x8\"\n",
    "]\n",
    "\n",
    "feature_extractor_net_cfgs: Dict[str, List[Union[str, int]]] = {\n",
    "    \"vgg11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"vgg13\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"vgg16\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n",
    "    \"vgg19\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"],\n",
    "}\n",
    "\n",
    "\n",
    "def _make_layers(net_cfg_name: str, batch_norm: bool = False) -> nn.Sequential:\n",
    "    net_cfg = feature_extractor_net_cfgs[net_cfg_name]\n",
    "    layers: nn.Sequential[nn.Module] = nn.Sequential()\n",
    "    in_channels = 3\n",
    "    for v in net_cfg:\n",
    "        if v == \"M\":\n",
    "            layers.append(nn.MaxPool2d((2, 2), (2, 2)))\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = nn.Conv2d(in_channels, v, (3, 3), (1, 1), (1, 1))\n",
    "            if batch_norm:\n",
    "                layers.append(conv2d)\n",
    "                layers.append(nn.BatchNorm2d(v))\n",
    "                layers.append(nn.ReLU(True))\n",
    "            else:\n",
    "                layers.append(conv2d)\n",
    "                layers.append(nn.ReLU(True))\n",
    "            in_channels = v\n",
    "\n",
    "    return layers\n",
    "\n",
    "\n",
    "class _FeatureExtractor(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            net_cfg_name: str = \"vgg19\",\n",
    "            batch_norm: bool = False,\n",
    "            num_classes: int = 1000) -> None:\n",
    "        super(_FeatureExtractor, self).__init__()\n",
    "        self.features = _make_layers(net_cfg_name, batch_norm)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "        # Initialize neural network weights\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, 0, 0.01)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "    # Support torch.script function\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class RRDBNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int = 3,\n",
    "            out_channels: int = 3,\n",
    "            channels: int = 64,\n",
    "            growth_channels: int = 32,\n",
    "            num_rrdb: int = 23,\n",
    "            upscale: int = 4,\n",
    "    ) -> None:\n",
    "        super(RRDBNet, self).__init__()\n",
    "        self.upscale = upscale\n",
    "\n",
    "        # The first layer of convolutional layer.\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, (3, 3), (1, 1), (1, 1))\n",
    "\n",
    "        # Feature extraction backbone network.\n",
    "        trunk = []\n",
    "        for _ in range(num_rrdb):\n",
    "            trunk.append(_ResidualResidualDenseBlock(channels, growth_channels))\n",
    "        self.trunk = nn.Sequential(*trunk)\n",
    "\n",
    "        # After the feature extraction network, reconnect a layer of convolutional blocks.\n",
    "        self.conv2 = nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1))\n",
    "\n",
    "        # Upsampling convolutional layer.\n",
    "        if upscale == 2:\n",
    "            self.upsampling1 = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1)),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            )\n",
    "        if upscale == 4:\n",
    "            self.upsampling1 = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1)),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            )\n",
    "            self.upsampling2 = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1)),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            )\n",
    "        if upscale == 8:\n",
    "            self.upsampling1 = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1)),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            )\n",
    "            self.upsampling2 = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1)),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            )\n",
    "            self.upsampling3 = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1)),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            )\n",
    "\n",
    "        # Reconnect a layer of convolution block after upsampling.\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1)),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "\n",
    "        # Output layer.\n",
    "        self.conv4 = nn.Conv2d(channels, out_channels, (3, 3), (1, 1), (1, 1))\n",
    "\n",
    "        # Initialize all layer\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                module.weight.data *= 0.2\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    # The model should be defined in the Torch.script method.\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        conv1 = self.conv1(x)\n",
    "        x = self.trunk(conv1)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.add(x, conv1)\n",
    "\n",
    "        if self.upscale == 2:\n",
    "            x = self.upsampling1(F_torch.interpolate(x, scale_factor=2, mode=\"nearest\"))\n",
    "        if self.upscale == 4:\n",
    "            x = self.upsampling1(F_torch.interpolate(x, scale_factor=2, mode=\"nearest\"))\n",
    "            x = self.upsampling2(F_torch.interpolate(x, scale_factor=2, mode=\"nearest\"))\n",
    "        if self.upscale == 8:\n",
    "            x = self.upsampling1(F_torch.interpolate(x, scale_factor=2, mode=\"nearest\"))\n",
    "            x = self.upsampling2(F_torch.interpolate(x, scale_factor=2, mode=\"nearest\"))\n",
    "            x = self.upsampling3(F_torch.interpolate(x, scale_factor=2, mode=\"nearest\"))\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "class _ResidualDenseBlock(nn.Module):\n",
    "    \"\"\"Achieves densely connected convolutional layers.\n",
    "    `Densely Connected Convolutional Networks <https://arxiv.org/pdf/1608.06993v5.pdf>` paper.\n",
    "\n",
    "    Args:\n",
    "        channels (int): The number of channels in the input image.\n",
    "        growth_channels (int): The number of channels that increase in each layer of convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, growth_channels: int) -> None:\n",
    "        super(_ResidualDenseBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + growth_channels * 0, growth_channels, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv2 = nn.Conv2d(channels + growth_channels * 1, growth_channels, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(channels + growth_channels * 2, growth_channels, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(channels + growth_channels * 3, growth_channels, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv5 = nn.Conv2d(channels + growth_channels * 4, channels, (3, 3), (1, 1), (1, 1))\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2, True)\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out1 = self.leaky_relu(self.conv1(x))\n",
    "        out2 = self.leaky_relu(self.conv2(torch.cat([x, out1], 1)))\n",
    "        out3 = self.leaky_relu(self.conv3(torch.cat([x, out1, out2], 1)))\n",
    "        out4 = self.leaky_relu(self.conv4(torch.cat([x, out1, out2, out3], 1)))\n",
    "        out5 = self.identity(self.conv5(torch.cat([x, out1, out2, out3, out4], 1)))\n",
    "\n",
    "        x = torch.mul(out5, 0.2)\n",
    "        x = torch.add(x, identity)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class _ResidualResidualDenseBlock(nn.Module):\n",
    "    \"\"\"Multi-layer residual dense convolution block.\n",
    "\n",
    "    Args:\n",
    "        channels (int): The number of channels in the input image.\n",
    "        growth_channels (int): The number of channels that increase in each layer of convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, growth_channels: int) -> None:\n",
    "        super(_ResidualResidualDenseBlock, self).__init__()\n",
    "        self.rdb1 = _ResidualDenseBlock(channels, growth_channels)\n",
    "        self.rdb2 = _ResidualDenseBlock(channels, growth_channels)\n",
    "        self.rdb3 = _ResidualDenseBlock(channels, growth_channels)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        x = self.rdb1(x)\n",
    "        x = self.rdb2(x)\n",
    "        x = self.rdb3(x)\n",
    "\n",
    "        x = torch.mul(x, 0.2)\n",
    "        x = torch.add(x, identity)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiscriminatorForVGG(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int = 3,\n",
    "            out_channels: int = 3,\n",
    "            channels: int = 64,\n",
    "    ) -> None:\n",
    "        super(DiscriminatorForVGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # input size. (3) x 128 x 128\n",
    "            nn.Conv2d(in_channels, channels, (3, 3), (1, 1), (1, 1), bias=True),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size. (64) x 64 x 64\n",
    "            nn.Conv2d(channels, channels, (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(channels, int(2 * channels), (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(int(2 * channels)),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size. (128) x 32 x 32\n",
    "            nn.Conv2d(int(2 * channels), int(2 * channels), (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(int(2 * channels)),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(int(2 * channels), int(4 * channels), (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(int(4 * channels)),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size. (256) x 16 x 16\n",
    "            nn.Conv2d(int(4 * channels), int(4 * channels), (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(int(4 * channels)),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(int(4 * channels), int(8 * channels), (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(int(8 * channels)),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size. (512) x 8 x 8\n",
    "            nn.Conv2d(int(8 * channels), int(8 * channels), (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(int(8 * channels)),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(int(8 * channels), int(8 * channels), (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(int(8 * channels)),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size. (512) x 4 x 4\n",
    "            nn.Conv2d(int(8 * channels), int(8 * channels), (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(int(8 * channels)),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(int(8 * channels) * 4 * 4, 100),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(100, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.features(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    \"\"\"Constructs a content loss function based on the VGG19 network.\n",
    "    Using high-level feature mapping layers from the latter layers will focus more on the texture content of the image.\n",
    "\n",
    "    Paper reference list:\n",
    "        -`Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network <https://arxiv.org/pdf/1609.04802.pdf>` paper.\n",
    "        -`ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks                    <https://arxiv.org/pdf/1809.00219.pdf>` paper.\n",
    "        -`Perceptual Extreme Super Resolution Network with Receptive Field Block               <https://arxiv.org/pdf/2005.12597.pdf>` paper.\n",
    "\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            net_cfg_name: str = \"vgg19\",\n",
    "            batch_norm: bool = False,\n",
    "            num_classes: int = 1000,\n",
    "            model_weights_path: str = \"\",\n",
    "            feature_nodes: list = None,\n",
    "            feature_normalize_mean: list = None,\n",
    "            feature_normalize_std: list = None,\n",
    "    ) -> None:\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # Define the feature extraction model\n",
    "        model = _FeatureExtractor(net_cfg_name, batch_norm, num_classes)\n",
    "        # Load the pre-trained model\n",
    "        if model_weights_path == \"\":\n",
    "            model = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1)\n",
    "        elif model_weights_path is not None and os.path.exists(model_weights_path):\n",
    "            checkpoint = torch.load(model_weights_path, map_location=lambda storage, loc: storage)\n",
    "            if \"state_dict\" in checkpoint.keys():\n",
    "                model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Model weight file not found\")\n",
    "        # Extract the output of the feature extraction layer\n",
    "        self.feature_extractor = create_feature_extractor(model, feature_nodes)\n",
    "        # Select the specified layers as the feature extraction layer\n",
    "        self.feature_extractor_nodes = feature_nodes\n",
    "        # input normalization\n",
    "        self.normalize = transforms.Normalize(feature_normalize_mean, feature_normalize_std)\n",
    "        # Freeze model parameters without derivatives\n",
    "        for model_parameters in self.feature_extractor.parameters():\n",
    "            model_parameters.requires_grad = False\n",
    "        self.feature_extractor.eval()\n",
    "\n",
    "    def forward(self, sr_tensor: Tensor, gt_tensor: Tensor) -> [Tensor]:\n",
    "        assert sr_tensor.size() == gt_tensor.size(), \"Two tensor must have the same size\"\n",
    "        device = sr_tensor.device\n",
    "\n",
    "        losses = []\n",
    "        # input normalization\n",
    "        sr_tensor = self.normalize(sr_tensor)\n",
    "        gt_tensor = self.normalize(gt_tensor)\n",
    "\n",
    "        # Get the output of the feature extraction layer\n",
    "        sr_feature = self.feature_extractor(sr_tensor)\n",
    "        gt_feature = self.feature_extractor(gt_tensor)\n",
    "\n",
    "        # Compute feature loss\n",
    "        for i in range(len(self.feature_extractor_nodes)):\n",
    "            losses.append(F_torch.l1_loss(sr_feature[self.feature_extractor_nodes[i]],\n",
    "                                          gt_feature[self.feature_extractor_nodes[i]]))\n",
    "\n",
    "        losses = torch.Tensor([losses]).to(device)\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "def rrdbnet_x2(**kwargs: Any) -> RRDBNet:\n",
    "    model = RRDBNet(upscale=2, **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def rrdbnet_x4(**kwargs: Any) -> RRDBNet:\n",
    "    model = RRDBNet(upscale=4, **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def rrdbnet_x8(**kwargs: Any) -> RRDBNet:\n",
    "    model = RRDBNet(upscale=8, **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator_for_vgg(**kwargs) -> DiscriminatorForVGG:\n",
    "    model = DiscriminatorForVGG(**kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Dakewe Biotech Corporation. All Rights Reserved.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#   you may not use this file except in compliance with the License.\n",
    "#   You may obtain a copy of the License at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "import math\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy import ndarray\n",
    "from torch import Tensor\n",
    "from torchvision.transforms import functional as F_vision\n",
    "\n",
    "__all__ = [\n",
    "    \"image_to_tensor\", \"tensor_to_image\",\n",
    "    \"image_resize\", \"preprocess_one_image\",\n",
    "    \"expand_y\", \"rgb_to_ycbcr\", \"bgr_to_ycbcr\", \"ycbcr_to_bgr\", \"ycbcr_to_rgb\",\n",
    "    \"rgb_to_ycbcr_torch\", \"bgr_to_ycbcr_torch\",\n",
    "    \"center_crop\", \"random_crop\", \"random_rotate\", \"random_vertically_flip\", \"random_horizontally_flip\",\n",
    "    \"center_crop_torch\", \"random_crop_torch\", \"random_rotate_torch\", \"random_vertically_flip_torch\",\n",
    "    \"random_horizontally_flip_torch\",\n",
    "]\n",
    "\n",
    "\n",
    "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
    "def _cubic(x: Any) -> Any:\n",
    "    \"\"\"Implementation of `cubic` function in Matlab under Python language.\n",
    "\n",
    "    Args:\n",
    "        x: Element vector.\n",
    "\n",
    "    Returns:\n",
    "        Bicubic interpolation\n",
    "\n",
    "    \"\"\"\n",
    "    absx = torch.abs(x)\n",
    "    absx2 = absx ** 2\n",
    "    absx3 = absx ** 3\n",
    "    return (1.5 * absx3 - 2.5 * absx2 + 1) * ((absx <= 1).type_as(absx)) + (\n",
    "            -0.5 * absx3 + 2.5 * absx2 - 4 * absx + 2) * (\n",
    "        ((absx > 1) * (absx <= 2)).type_as(absx))\n",
    "\n",
    "\n",
    "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
    "def _calculate_weights_indices(in_length: int,\n",
    "                               out_length: int,\n",
    "                               scale: float,\n",
    "                               kernel_width: int,\n",
    "                               antialiasing: bool) -> [np.ndarray, np.ndarray, int, int]:\n",
    "    \"\"\"Implementation of `calculate_weights_indices` function in Matlab under Python language.\n",
    "\n",
    "    Args:\n",
    "        in_length (int): Input length.\n",
    "        out_length (int): Output length.\n",
    "        scale (float): Scale factor.\n",
    "        kernel_width (int): Kernel width.\n",
    "        antialiasing (bool): Whether to apply antialiasing when down-sampling operations.\n",
    "            Caution: Bicubic down-sampling in PIL uses antialiasing by default.\n",
    "\n",
    "    Returns:\n",
    "       weights, indices, sym_len_s, sym_len_e\n",
    "\n",
    "    \"\"\"\n",
    "    if (scale < 1) and antialiasing:\n",
    "        # Use a modified kernel (larger kernel width) to simultaneously\n",
    "        # interpolate and antialiasing\n",
    "        kernel_width = kernel_width / scale\n",
    "\n",
    "    # Output-space coordinates\n",
    "    x = torch.linspace(1, out_length, out_length)\n",
    "\n",
    "    # Input-space coordinates. Calculate the inverse mapping such that 0.5\n",
    "    # in output space maps to 0.5 in input space, and 0.5 + scale in output\n",
    "    # space maps to 1.5 in input space.\n",
    "    u = x / scale + 0.5 * (1 - 1 / scale)\n",
    "\n",
    "    # What is the left-most pixel that can be involved in the computation?\n",
    "    left = torch.floor(u - kernel_width / 2)\n",
    "\n",
    "    # What is the maximum number of pixels that can be involved in the\n",
    "    # computation?  Note: it's OK to use an extra pixel here; if the\n",
    "    # corresponding weights are all zero, it will be eliminated at the end\n",
    "    # of this function.\n",
    "    p = math.ceil(kernel_width) + 2\n",
    "\n",
    "    # The indices of the input pixels involved in computing the k-th output\n",
    "    # pixel are in row k of the indices matrix.\n",
    "    indices = left.view(out_length, 1).expand(out_length, p) + torch.linspace(0, p - 1, p).view(1, p).expand(\n",
    "        out_length, p)\n",
    "\n",
    "    # The weights used to compute the k-th output pixel are in row k of the\n",
    "    # weights matrix.\n",
    "    distance_to_center = u.view(out_length, 1).expand(out_length, p) - indices\n",
    "\n",
    "    # apply cubic kernel\n",
    "    if (scale < 1) and antialiasing:\n",
    "        weights = scale * _cubic(distance_to_center * scale)\n",
    "    else:\n",
    "        weights = _cubic(distance_to_center)\n",
    "\n",
    "    # Normalize the weights matrix so that each row sums to 1.\n",
    "    weights_sum = torch.sum(weights, 1).view(out_length, 1)\n",
    "    weights = weights / weights_sum.expand(out_length, p)\n",
    "\n",
    "    # If a column in weights is all zero, get rid of it. only consider the\n",
    "    # first and last column.\n",
    "    weights_zero_tmp = torch.sum((weights == 0), 0)\n",
    "    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):\n",
    "        indices = indices.narrow(1, 1, p - 2)\n",
    "        weights = weights.narrow(1, 1, p - 2)\n",
    "    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):\n",
    "        indices = indices.narrow(1, 0, p - 2)\n",
    "        weights = weights.narrow(1, 0, p - 2)\n",
    "    weights = weights.contiguous()\n",
    "    indices = indices.contiguous()\n",
    "    sym_len_s = -indices.min() + 1\n",
    "    sym_len_e = indices.max() - in_length\n",
    "    indices = indices + sym_len_s - 1\n",
    "    return weights, indices, int(sym_len_s), int(sym_len_e)\n",
    "\n",
    "\n",
    "def image_to_tensor(image: ndarray, range_norm: bool, half: bool) -> Tensor:\n",
    "    \"\"\"Convert the image data type to the Tensor (NCWH) data type supported by PyTorch\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The image data read by ``OpenCV.imread``, the data range is [0,255] or [0, 1]\n",
    "        range_norm (bool): Scale [0, 1] data to between [-1, 1]\n",
    "        half (bool): Whether to convert torch.float32 similarly to torch.half type\n",
    "\n",
    "    Returns:\n",
    "        tensor (Tensor): Data types supported by PyTorch\n",
    "\n",
    "    Examples:\n",
    "        >>> example_image = cv2.imread(\"lr_image.bmp\")\n",
    "        >>> example_tensor = image_to_tensor(example_image, range_norm=True, half=False)\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert image data type to Tensor data type\n",
    "    tensor = torch.from_numpy(np.ascontiguousarray(image)).permute(2, 0, 1).float()\n",
    "\n",
    "    # Scale the image data from [0, 1] to [-1, 1]\n",
    "    if range_norm:\n",
    "        tensor = tensor.mul(2.0).sub(1.0)\n",
    "\n",
    "    # Convert torch.float32 image data type to torch.half image data type\n",
    "    if half:\n",
    "        tensor = tensor.half()\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor_to_image(tensor: Tensor, range_norm: bool, half: bool) -> Any:\n",
    "    \"\"\"Convert the Tensor(NCWH) data type supported by PyTorch to the np.ndarray(WHC) image data type\n",
    "\n",
    "    Args:\n",
    "        tensor (Tensor): Data types supported by PyTorch (NCHW), the data range is [0, 1]\n",
    "        range_norm (bool): Scale [-1, 1] data to between [0, 1]\n",
    "        half (bool): Whether to convert torch.float32 similarly to torch.half type.\n",
    "\n",
    "    Returns:\n",
    "        image (np.ndarray): Data types supported by PIL or OpenCV\n",
    "\n",
    "    Examples:\n",
    "        >>> example_image = cv2.imread(\"lr_image.bmp\")\n",
    "        >>> example_tensor = image_to_tensor(example_image, range_norm=False, half=False)\n",
    "\n",
    "    \"\"\"\n",
    "    if range_norm:\n",
    "        tensor = tensor.add(1.0).div(2.0)\n",
    "    if half:\n",
    "        tensor = tensor.half()\n",
    "\n",
    "    image = tensor.squeeze(0).permute(1, 2, 0).mul(255).clamp(0, 255).cpu().numpy().astype(\"uint8\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess_one_image(image_path: str, range_norm: bool, half: bool, device: torch.device) -> Tensor:\n",
    "    # read an image using OpenCV\n",
    "    image = cv2.imread(image_path).astype(np.float32) / 255.0\n",
    "\n",
    "    # BGR image channel data to RGB image channel data\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert RGB image channel data to image formats supported by PyTorch\n",
    "    tensor = image_to_tensor(image, range_norm, half).unsqueeze_(0)\n",
    "\n",
    "    # Data transfer to the specified device\n",
    "    tensor = tensor.to(device, non_blocking=True)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
    "def image_resize(image: Any, scale_factor: float, antialiasing: bool = True) -> Any:\n",
    "    \"\"\"Implementation of `imresize` function in Matlab under Python language.\n",
    "\n",
    "    Args:\n",
    "        image: The input image.\n",
    "        scale_factor (float): Scale factor. The same scale applies for both height and width.\n",
    "        antialiasing (bool): Whether to apply antialiasing when down-sampling operations.\n",
    "            Caution: Bicubic down-sampling in `PIL` uses antialiasing by default. Default: ``True``.\n",
    "\n",
    "    Returns:\n",
    "        out_2 (np.ndarray): Output image with shape (c, h, w), [0, 1] range, w/o round\n",
    "\n",
    "    \"\"\"\n",
    "    squeeze_flag = False\n",
    "    if type(image).__module__ == np.__name__:  # numpy type\n",
    "        numpy_type = True\n",
    "        if image.ndim == 2:\n",
    "            image = image[:, :, None]\n",
    "            squeeze_flag = True\n",
    "        image = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
    "    else:\n",
    "        numpy_type = False\n",
    "        if image.ndim == 2:\n",
    "            image = image.unsqueeze(0)\n",
    "            squeeze_flag = True\n",
    "\n",
    "    in_c, in_h, in_w = image.size()\n",
    "    out_h, out_w = math.ceil(in_h * scale_factor), math.ceil(in_w * scale_factor)\n",
    "    kernel_width = 4\n",
    "\n",
    "    # get weights and indices\n",
    "    weights_h, indices_h, sym_len_hs, sym_len_he = _calculate_weights_indices(in_h, out_h, scale_factor, kernel_width,\n",
    "                                                                              antialiasing)\n",
    "    weights_w, indices_w, sym_len_ws, sym_len_we = _calculate_weights_indices(in_w, out_w, scale_factor, kernel_width,\n",
    "                                                                              antialiasing)\n",
    "    # process H dimension\n",
    "    # symmetric copying\n",
    "    img_aug = torch.FloatTensor(in_c, in_h + sym_len_hs + sym_len_he, in_w)\n",
    "    img_aug.narrow(1, sym_len_hs, in_h).copy_(image)\n",
    "\n",
    "    sym_patch = image[:, :sym_len_hs, :]\n",
    "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
    "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
    "    img_aug.narrow(1, 0, sym_len_hs).copy_(sym_patch_inv)\n",
    "\n",
    "    sym_patch = image[:, -sym_len_he:, :]\n",
    "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
    "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
    "    img_aug.narrow(1, sym_len_hs + in_h, sym_len_he).copy_(sym_patch_inv)\n",
    "\n",
    "    out_1 = torch.FloatTensor(in_c, out_h, in_w)\n",
    "    kernel_width = weights_h.size(1)\n",
    "    for i in range(out_h):\n",
    "        idx = int(indices_h[i][0])\n",
    "        for j in range(in_c):\n",
    "            out_1[j, i, :] = img_aug[j, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_h[i])\n",
    "\n",
    "    # process W dimension\n",
    "    # symmetric copying\n",
    "    out_1_aug = torch.FloatTensor(in_c, out_h, in_w + sym_len_ws + sym_len_we)\n",
    "    out_1_aug.narrow(2, sym_len_ws, in_w).copy_(out_1)\n",
    "\n",
    "    sym_patch = out_1[:, :, :sym_len_ws]\n",
    "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
    "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
    "    out_1_aug.narrow(2, 0, sym_len_ws).copy_(sym_patch_inv)\n",
    "\n",
    "    sym_patch = out_1[:, :, -sym_len_we:]\n",
    "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
    "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
    "    out_1_aug.narrow(2, sym_len_ws + in_w, sym_len_we).copy_(sym_patch_inv)\n",
    "\n",
    "    out_2 = torch.FloatTensor(in_c, out_h, out_w)\n",
    "    kernel_width = weights_w.size(1)\n",
    "    for i in range(out_w):\n",
    "        idx = int(indices_w[i][0])\n",
    "        for j in range(in_c):\n",
    "            out_2[j, :, i] = out_1_aug[j, :, idx:idx + kernel_width].mv(weights_w[i])\n",
    "\n",
    "    if squeeze_flag:\n",
    "        out_2 = out_2.squeeze(0)\n",
    "    if numpy_type:\n",
    "        out_2 = out_2.numpy()\n",
    "        if not squeeze_flag:\n",
    "            out_2 = out_2.transpose(1, 2, 0)\n",
    "\n",
    "    return out_2\n",
    "\n",
    "\n",
    "def expand_y(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert BGR channel to YCbCr format,\n",
    "    and expand Y channel data in YCbCr, from HW to HWC\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Y channel image data\n",
    "\n",
    "    Returns:\n",
    "        y_image (np.ndarray): Y-channel image data in HWC form\n",
    "\n",
    "    \"\"\"\n",
    "    # Normalize image data to [0, 1]\n",
    "    image = image.astype(np.float32) / 255.\n",
    "\n",
    "    # Convert BGR to YCbCr, and extract only Y channel\n",
    "    y_image = bgr_to_ycbcr(image, only_use_y_channel=True)\n",
    "\n",
    "    # Expand Y channel\n",
    "    y_image = y_image[..., None]\n",
    "\n",
    "    # Normalize the image data to [0, 255]\n",
    "    y_image = y_image.astype(np.float64) * 255.0\n",
    "\n",
    "    return y_image\n",
    "\n",
    "\n",
    "def rgb_to_ycbcr(image: np.ndarray, only_use_y_channel: bool) -> np.ndarray:\n",
    "    \"\"\"Implementation of rgb2ycbcr function in Matlab under Python language\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Image input in RGB format.\n",
    "        only_use_y_channel (bool): Extract Y channel separately\n",
    "\n",
    "    Returns:\n",
    "        image (np.ndarray): YCbCr image array data\n",
    "\n",
    "    \"\"\"\n",
    "    if only_use_y_channel:\n",
    "        image = np.dot(image, [65.481, 128.553, 24.966]) + 16.0\n",
    "    else:\n",
    "        image = np.matmul(image, [[65.481, -37.797, 112.0], [128.553, -74.203, -93.786], [24.966, 112.0, -18.214]]) + [\n",
    "            16, 128, 128]\n",
    "\n",
    "    image /= 255.\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def bgr_to_ycbcr(image: np.ndarray, only_use_y_channel: bool) -> np.ndarray:\n",
    "    \"\"\"Implementation of bgr2ycbcr function in Matlab under Python language.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Image input in BGR format\n",
    "        only_use_y_channel (bool): Extract Y channel separately\n",
    "\n",
    "    Returns:\n",
    "        image (np.ndarray): YCbCr image array data\n",
    "\n",
    "    \"\"\"\n",
    "    if only_use_y_channel:\n",
    "        image = np.dot(image, [24.966, 128.553, 65.481]) + 16.0\n",
    "    else:\n",
    "        image = np.matmul(image, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786], [65.481, -37.797, 112.0]]) + [\n",
    "            16, 128, 128]\n",
    "\n",
    "    image /= 255.\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def ycbcr_to_rgb(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Implementation of ycbcr2rgb function in Matlab under Python language.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Image input in YCbCr format.\n",
    "\n",
    "    Returns:\n",
    "        image (np.ndarray): RGB image array data\n",
    "\n",
    "    \"\"\"\n",
    "    image_dtype = image.dtype\n",
    "    image *= 255.\n",
    "\n",
    "    image = np.matmul(image, [[0.00456621, 0.00456621, 0.00456621],\n",
    "                              [0, -0.00153632, 0.00791071],\n",
    "                              [0.00625893, -0.00318811, 0]]) * 255.0 + [-222.921, 135.576, -276.836]\n",
    "\n",
    "    image /= 255.\n",
    "    image = image.astype(image_dtype)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def ycbcr_to_bgr(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Implementation of ycbcr2bgr function in Matlab under Python language.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Image input in YCbCr format.\n",
    "\n",
    "    Returns:\n",
    "        image (np.ndarray): BGR image array data\n",
    "\n",
    "    \"\"\"\n",
    "    image_dtype = image.dtype\n",
    "    image *= 255.\n",
    "\n",
    "    image = np.matmul(image, [[0.00456621, 0.00456621, 0.00456621],\n",
    "                              [0.00791071, -0.00153632, 0],\n",
    "                              [0, -0.00318811, 0.00625893]]) * 255.0 + [-276.836, 135.576, -222.921]\n",
    "\n",
    "    image /= 255.\n",
    "    image = image.astype(image_dtype)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def rgb_to_ycbcr_torch(tensor: Tensor, only_use_y_channel: bool) -> Tensor:\n",
    "    \"\"\"Implementation of rgb2ycbcr function in Matlab under PyTorch\n",
    "\n",
    "    References from：`https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion`\n",
    "\n",
    "    Args:\n",
    "        tensor (Tensor): Image data in PyTorch format\n",
    "        only_use_y_channel (bool): Extract only Y channel\n",
    "\n",
    "    Returns:\n",
    "        tensor (Tensor): YCbCr image data in PyTorch format\n",
    "\n",
    "    \"\"\"\n",
    "    if only_use_y_channel:\n",
    "        weight = Tensor([[65.481], [128.553], [24.966]]).to(tensor)\n",
    "        tensor = torch.matmul(tensor.permute(0, 2, 3, 1), weight).permute(0, 3, 1, 2) + 16.0\n",
    "    else:\n",
    "        weight = Tensor([[65.481, -37.797, 112.0],\n",
    "                         [128.553, -74.203, -93.786],\n",
    "                         [24.966, 112.0, -18.214]]).to(tensor)\n",
    "        bias = Tensor([16, 128, 128]).view(1, 3, 1, 1).to(tensor)\n",
    "        tensor = torch.matmul(tensor.permute(0, 2, 3, 1), weight).permute(0, 3, 1, 2) + bias\n",
    "\n",
    "    tensor /= 255.\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def bgr_to_ycbcr_torch(tensor: Tensor, only_use_y_channel: bool) -> Tensor:\n",
    "    \"\"\"Implementation of bgr2ycbcr function in Matlab under PyTorch\n",
    "\n",
    "    References from：`https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion`\n",
    "\n",
    "    Args:\n",
    "        tensor (Tensor): Image data in PyTorch format\n",
    "        only_use_y_channel (bool): Extract only Y channel\n",
    "\n",
    "    Returns:\n",
    "        tensor (Tensor): YCbCr image data in PyTorch format\n",
    "\n",
    "    \"\"\"\n",
    "    if only_use_y_channel:\n",
    "        weight = Tensor([[24.966], [128.553], [65.481]]).to(tensor)\n",
    "        tensor = torch.matmul(tensor.permute(0, 2, 3, 1), weight).permute(0, 3, 1, 2) + 16.0\n",
    "    else:\n",
    "        weight = Tensor([[24.966, 112.0, -18.214],\n",
    "                         [128.553, -74.203, -93.786],\n",
    "                         [65.481, -37.797, 112.0]]).to(tensor)\n",
    "        bias = Tensor([16, 128, 128]).view(1, 3, 1, 1).to(tensor)\n",
    "        tensor = torch.matmul(tensor.permute(0, 2, 3, 1), weight).permute(0, 3, 1, 2) + bias\n",
    "\n",
    "    tensor /= 255.\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def center_crop(image: np.ndarray, image_size: int) -> np.ndarray:\n",
    "    \"\"\"Crop small image patches from one image center area.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The input image for `OpenCV.imread`.\n",
    "        image_size (int): The size of the captured image area.\n",
    "\n",
    "    Returns:\n",
    "        patch_image (np.ndarray): Small patch image\n",
    "\n",
    "    \"\"\"\n",
    "    image_height, image_width = image.shape[:2]\n",
    "\n",
    "    # Just need to find the top and left coordinates of the image\n",
    "    top = (image_height - image_size) // 2\n",
    "    left = (image_width - image_size) // 2\n",
    "\n",
    "    # Crop image patch\n",
    "    patch_image = image[top:top + image_size, left:left + image_size, ...]\n",
    "\n",
    "    return patch_image\n",
    "\n",
    "\n",
    "def random_crop(image: np.ndarray, image_size: int) -> np.ndarray:\n",
    "    \"\"\"Crop small image patches from one image.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The input image for `OpenCV.imread`.\n",
    "        image_size (int): The size of the captured image area.\n",
    "\n",
    "    Returns:\n",
    "        patch_image (np.ndarray): Small patch image\n",
    "\n",
    "    \"\"\"\n",
    "    image_height, image_width = image.shape[:2]\n",
    "\n",
    "    # Just need to find the top and left coordinates of the image\n",
    "    top = random.randint(0, image_height - image_size)\n",
    "    left = random.randint(0, image_width - image_size)\n",
    "\n",
    "    # Crop image patch\n",
    "    patch_image = image[top:top + image_size, left:left + image_size, ...]\n",
    "\n",
    "    return patch_image\n",
    "\n",
    "\n",
    "def random_rotate(image,\n",
    "                  angles: list,\n",
    "                  center: tuple[int, int] = None,\n",
    "                  scale_factor: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Rotate an image by a random angle\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Image read with OpenCV\n",
    "        angles (list): Rotation angle range\n",
    "        center (optional, tuple[int, int]): High resolution image selection center point. Default: ``None``\n",
    "        scale_factor (optional, float): scaling factor. Default: 1.0\n",
    "\n",
    "    Returns:\n",
    "        rotated_image (np.ndarray): image after rotation\n",
    "\n",
    "    \"\"\"\n",
    "    image_height, image_width = image.shape[:2]\n",
    "\n",
    "    if center is None:\n",
    "        center = (image_width // 2, image_height // 2)\n",
    "\n",
    "    # Random select specific angle\n",
    "    angle = random.choice(angles)\n",
    "    matrix = cv2.getRotationMatrix2D(center, angle, scale_factor)\n",
    "    rotated_image = cv2.warpAffine(image, matrix, (image_width, image_height))\n",
    "\n",
    "    return rotated_image\n",
    "\n",
    "\n",
    "def random_horizontally_flip(image: np.ndarray, p: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"Flip the image upside down randomly\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Image read with OpenCV\n",
    "        p (optional, float): Horizontally flip probability. Default: 0.5\n",
    "\n",
    "    Returns:\n",
    "        horizontally_flip_image (np.ndarray): image after horizontally flip\n",
    "\n",
    "    \"\"\"\n",
    "    if random.random() < p:\n",
    "        horizontally_flip_image = cv2.flip(image, 1)\n",
    "    else:\n",
    "        horizontally_flip_image = image\n",
    "\n",
    "    return horizontally_flip_image\n",
    "\n",
    "\n",
    "def random_vertically_flip(image: np.ndarray, p: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"Flip an image horizontally randomly\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Image read with OpenCV\n",
    "        p (optional, float): Vertically flip probability. Default: 0.5\n",
    "\n",
    "    Returns:\n",
    "        vertically_flip_image (np.ndarray): image after vertically flip\n",
    "\n",
    "    \"\"\"\n",
    "    if random.random() < p:\n",
    "        vertically_flip_image = cv2.flip(image, 0)\n",
    "    else:\n",
    "        vertically_flip_image = image\n",
    "\n",
    "    return vertically_flip_image\n",
    "\n",
    "\n",
    "def center_crop_torch(\n",
    "        gt_images: ndarray | Tensor | list[ndarray] | list[Tensor],\n",
    "        lr_images: ndarray | Tensor | list[ndarray] | list[Tensor],\n",
    "        gt_patch_size: int,\n",
    "        upscale_factor: int,\n",
    ") -> [ndarray, ndarray] or [Tensor, Tensor] or [list[ndarray], list[ndarray]] or [list[Tensor], list[Tensor]]:\n",
    "    \"\"\"Intercept two images to specify the center area\n",
    "\n",
    "    Args:\n",
    "        gt_images (ndarray | Tensor | list[ndarray] | list[Tensor]): ground truth images read by PyTorch\n",
    "        lr_images (ndarray | Tensor | list[ndarray] | list[Tensor]): Low resolution images read by PyTorch\n",
    "        gt_patch_size (int): the size of the ground truth image after interception\n",
    "        upscale_factor (int): the ground truth image size is a magnification of the low resolution image size\n",
    "\n",
    "    Returns:\n",
    "        gt_images (ndarray or Tensor or): the intercepted ground truth image\n",
    "        lr_images (ndarray or Tensor or): low-resolution intercepted images\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(gt_images, list):\n",
    "        gt_images = [gt_images]\n",
    "    if not isinstance(lr_images, list):\n",
    "        lr_images = [lr_images]\n",
    "\n",
    "    # detect input image type\n",
    "    input_type = \"Tensor\" if torch.is_tensor(lr_images[0]) else \"Numpy\"\n",
    "\n",
    "    if input_type == \"Tensor\":\n",
    "        lr_image_height, lr_image_width = lr_images[0].size()[-2:]\n",
    "    else:\n",
    "        lr_image_height, lr_image_width = lr_images[0].shape[0:2]\n",
    "\n",
    "    # Calculate the size of the low-resolution image that needs to be intercepted\n",
    "    lr_patch_size = gt_patch_size // upscale_factor\n",
    "\n",
    "    # Just need to find the top and left coordinates of the image\n",
    "    lr_top = (lr_image_height - lr_patch_size) // 2\n",
    "    lr_left = (lr_image_width - lr_patch_size) // 2\n",
    "\n",
    "    # Capture low-resolution images\n",
    "    if input_type == \"Tensor\":\n",
    "        lr_images = [lr_image[\n",
    "                     :,\n",
    "                     :,\n",
    "                     lr_top: lr_top + lr_patch_size,\n",
    "                     lr_left: lr_left + lr_patch_size] for lr_image in lr_images]\n",
    "    else:\n",
    "        lr_images = [lr_image[\n",
    "                     lr_top: lr_top + lr_patch_size,\n",
    "                     lr_left: lr_left + lr_patch_size,\n",
    "                     ...] for lr_image in lr_images]\n",
    "\n",
    "    # Intercept the ground truth image\n",
    "    gt_top, gt_left = int(lr_top * upscale_factor), int(lr_left * upscale_factor)\n",
    "\n",
    "    if input_type == \"Tensor\":\n",
    "        gt_images = [v[\n",
    "                     :,\n",
    "                     :,\n",
    "                     gt_top: gt_top + gt_patch_size,\n",
    "                     gt_left: gt_left + gt_patch_size] for v in gt_images]\n",
    "    else:\n",
    "        gt_images = [v[\n",
    "                     gt_top: gt_top + gt_patch_size,\n",
    "                     gt_left: gt_left + gt_patch_size,\n",
    "                     ...] for v in gt_images]\n",
    "\n",
    "    # When the input has only one image\n",
    "    if len(gt_images) == 1:\n",
    "        gt_images = gt_images[0]\n",
    "    if len(lr_images) == 1:\n",
    "        lr_images = lr_images[0]\n",
    "\n",
    "    return gt_images, lr_images\n",
    "\n",
    "\n",
    "def random_crop_torch(\n",
    "        gt_images: ndarray | Tensor | list[ndarray] | list[Tensor],\n",
    "        lr_images: ndarray | Tensor | list[ndarray] | list[Tensor],\n",
    "        gt_patch_size: int,\n",
    "        upscale_factor: int,\n",
    ") -> [ndarray, ndarray] or [Tensor, Tensor] or [list[ndarray], list[ndarray]] or [list[Tensor], list[Tensor]]:\n",
    "    \"\"\"Randomly intercept two images in the specified area\n",
    "\n",
    "    Args:\n",
    "        gt_images (ndarray | Tensor | list[ndarray] | list[Tensor]): ground truth images read by PyTorch\n",
    "        lr_images (ndarray | Tensor | list[ndarray] | list[Tensor]): Low resolution images read by PyTorch\n",
    "        gt_patch_size (int): the size of the ground truth image after interception\n",
    "        upscale_factor (int): the ground truth image size is a magnification of the low resolution image size\n",
    "\n",
    "    Returns:\n",
    "        gt_images (ndarray or Tensor or): the intercepted ground truth image\n",
    "        lr_images (ndarray or Tensor or): low-resolution intercepted images\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(gt_images, list):\n",
    "        gt_images = [gt_images]\n",
    "    if not isinstance(lr_images, list):\n",
    "        lr_images = [lr_images]\n",
    "\n",
    "    # detect input image type\n",
    "    input_type = \"Tensor\" if torch.is_tensor(lr_images[0]) else \"Numpy\"\n",
    "\n",
    "    if input_type == \"Tensor\":\n",
    "        lr_image_height, lr_image_width = lr_images[0].size()[-2:]\n",
    "    else:\n",
    "        lr_image_height, lr_image_width = lr_images[0].shape[0:2]\n",
    "\n",
    "    # Calculate the size of the low-resolution image that needs to be intercepted\n",
    "    lr_patch_size = gt_patch_size // upscale_factor\n",
    "\n",
    "    # Just need to find the top and left coordinates of the image\n",
    "    lr_top = random.randint(0, lr_image_height - lr_patch_size)\n",
    "    lr_left = random.randint(0, lr_image_width - lr_patch_size)\n",
    "\n",
    "    # Capture low-resolution images\n",
    "    if input_type == \"Tensor\":\n",
    "        lr_images = [lr_image[\n",
    "                     :,\n",
    "                     :,\n",
    "                     lr_top: lr_top + lr_patch_size,\n",
    "                     lr_left: lr_left + lr_patch_size] for lr_image in lr_images]\n",
    "    else:\n",
    "        lr_images = [lr_image[\n",
    "                     lr_top: lr_top + lr_patch_size,\n",
    "                     lr_left: lr_left + lr_patch_size,\n",
    "                     ...] for lr_image in lr_images]\n",
    "\n",
    "    # Intercept the ground truth image\n",
    "    gt_top, gt_left = int(lr_top * upscale_factor), int(lr_left * upscale_factor)\n",
    "\n",
    "    if input_type == \"Tensor\":\n",
    "        gt_images = [v[\n",
    "                     :,\n",
    "                     :,\n",
    "                     gt_top: gt_top + gt_patch_size,\n",
    "                     gt_left: gt_left + gt_patch_size] for v in gt_images]\n",
    "    else:\n",
    "        gt_images = [v[\n",
    "                     gt_top: gt_top + gt_patch_size,\n",
    "                     gt_left: gt_left + gt_patch_size,\n",
    "                     ...] for v in gt_images]\n",
    "\n",
    "    # When the input has only one image\n",
    "    if len(gt_images) == 1:\n",
    "        gt_images = gt_images[0]\n",
    "    if len(lr_images) == 1:\n",
    "        lr_images = lr_images[0]\n",
    "\n",
    "    return gt_images, lr_images\n",
    "\n",
    "\n",
    "def random_rotate_torch(\n",
    "        gt_images: ndarray | Tensor | list[ndarray] | list[Tensor],\n",
    "        lr_images: ndarray | Tensor | list[ndarray] | list[Tensor],\n",
    "        upscale_factor: int,\n",
    "        angles: list,\n",
    "        gt_center: tuple = None,\n",
    "        lr_center: tuple = None,\n",
    "        rotate_scale_factor: float = 1.0\n",
    ") -> [ndarray, ndarray] or [Tensor, Tensor] or [list[ndarray], list[ndarray]] or [list[Tensor], list[Tensor]]:\n",
    "    \"\"\"Randomly rotate the image\n",
    "\n",
    "    Args:\n",
    "        gt_images (ndarray | Tensor | list[ndarray] | list[Tensor]): ground truth images read by the PyTorch library\n",
    "        lr_images (ndarray | Tensor | list[ndarray] | list[Tensor]): low-resolution images read by the PyTorch library\n",
    "        angles (list): List of random rotation angles\n",
    "        upscale_factor (int): the ground truth image size is a magnification of the low resolution image size\n",
    "        gt_center (optional, tuple[int, int]): The center point of the ground truth image selection. Default: ``None``\n",
    "        lr_center (optional, tuple[int, int]): Low resolution image selection center point. Default: ``None``\n",
    "        rotate_scale_factor (optional, float): Rotation scaling factor. Default: 1.0\n",
    "\n",
    "    Returns:\n",
    "        gt_images (ndarray or Tensor or): ground truth image after rotation\n",
    "        lr_images (ndarray or Tensor or): Rotated low-resolution images\n",
    "\n",
    "    \"\"\"\n",
    "    # Randomly choose the rotation angle\n",
    "    angle = random.choice(angles)\n",
    "\n",
    "    if not isinstance(gt_images, list):\n",
    "        gt_images = [gt_images]\n",
    "    if not isinstance(lr_images, list):\n",
    "        lr_images = [lr_images]\n",
    "\n",
    "    # detect input image type\n",
    "    input_type = \"Tensor\" if torch.is_tensor(lr_images[0]) else \"Numpy\"\n",
    "\n",
    "    if input_type == \"Tensor\":\n",
    "        lr_image_height, lr_image_width = lr_images[0].size()[-2:]\n",
    "    else:\n",
    "        lr_image_height, lr_image_width = lr_images[0].shape[0:2]\n",
    "\n",
    "    # Rotate the low-res image\n",
    "    if lr_center is None:\n",
    "        lr_center = [lr_image_width // 2, lr_image_height // 2]\n",
    "\n",
    "    lr_matrix = cv2.getRotationMatrix2D(lr_center, angle, rotate_scale_factor)\n",
    "\n",
    "    if input_type == \"Tensor\":\n",
    "        lr_images = [F_vision.rotate(lr_image, angle, center=lr_center) for lr_image in lr_images]\n",
    "    else:\n",
    "        lr_images = [cv2.warpAffine(lr_image, lr_matrix, (lr_image_width, lr_image_height)) for lr_image in lr_images]\n",
    "\n",
    "    # rotate the ground truth image\n",
    "    gt_image_width = int(lr_image_width * upscale_factor)\n",
    "    gt_image_height = int(lr_image_height * upscale_factor)\n",
    "\n",
    "    if gt_center is None:\n",
    "        gt_center = [gt_image_width // 2, gt_image_height // 2]\n",
    "\n",
    "    gt_matrix = cv2.getRotationMatrix2D(gt_center, angle, rotate_scale_factor)\n",
    "\n",
    "    if input_type == \"Tensor\":\n",
    "        gt_images = [F_vision.rotate(gt_image, angle, center=gt_center) for gt_image in gt_images]\n",
    "    else:\n",
    "        gt_images = [cv2.warpAffine(gt_image, gt_matrix, (gt_image_width, gt_image_height)) for gt_image in gt_images]\n",
    "\n",
    "    # When the input has only one image\n",
    "    if len(gt_images) == 1:\n",
    "        gt_images = gt_images[0]\n",
    "    if len(lr_images) == 1:\n",
    "        lr_images = lr_images[0]\n",
    "\n",
    "    return gt_images, lr_images\n",
    "\n",
    "\n",
    "def random_horizontally_flip_torch(\n",
    "        gt_images: ndarray | Tensor | list[ndarray] | list[Tensor],\n",
    "        lr_images: ndarray | Tensor | list[ndarray] | list[Tensor],\n",
    "        p: float = 0.5\n",
    ") -> [ndarray, ndarray] or [Tensor, Tensor] or [list[ndarray], list[ndarray]] or [list[Tensor], list[Tensor]]:\n",
    "    \"\"\"Randomly flip the image up and down\n",
    "\n",
    "    Args:\n",
    "        gt_images (ndarray): ground truth images read by the PyTorch library\n",
    "        lr_images (ndarray): low resolution images read by the PyTorch library\n",
    "        p (optional, float): flip probability. Default: 0.5\n",
    "\n",
    "    Returns:\n",
    "        gt_images (ndarray or Tensor or): flipped ground truth images\n",
    "        lr_images (ndarray or Tensor or): flipped low-resolution images\n",
    "\n",
    "    \"\"\"\n",
    "    # Randomly generate flip probability\n",
    "    flip_prob = random.random()\n",
    "\n",
    "    if not isinstance(gt_images, list):\n",
    "        gt_images = [gt_images]\n",
    "    if not isinstance(lr_images, list):\n",
    "        lr_images = [lr_images]\n",
    "\n",
    "    # detect input image type\n",
    "    input_type = \"Tensor\" if torch.is_tensor(lr_images[0]) else \"Numpy\"\n",
    "\n",
    "    if flip_prob > p:\n",
    "        if input_type == \"Tensor\":\n",
    "            lr_images = [F_vision.hflip(lr_image) for lr_image in lr_images]\n",
    "            gt_images = [F_vision.hflip(gt_image) for gt_image in gt_images]\n",
    "        else:\n",
    "            lr_images = [cv2.flip(lr_image, 1) for lr_image in lr_images]\n",
    "            gt_images = [cv2.flip(gt_image, 1) for gt_image in gt_images]\n",
    "\n",
    "    # When the input has only one image\n",
    "    if len(gt_images) == 1:\n",
    "        gt_images = gt_images[0]\n",
    "    if len(lr_images) == 1:\n",
    "        lr_images = lr_images[0]\n",
    "\n",
    "    return gt_images, lr_images\n",
    "\n",
    "\n",
    "def random_vertically_flip_torch(\n",
    "        gt_images: ndarray | Tensor | list[ndarray] | list[Tensor],\n",
    "        lr_images: ndarray | Tensor | list[ndarray] | list[Tensor],\n",
    "        p: float = 0.5\n",
    ") -> [ndarray, ndarray] or [Tensor, Tensor] or [list[ndarray], list[ndarray]] or [list[Tensor], list[Tensor]]:\n",
    "    \"\"\"Randomly flip the image left and right\n",
    "\n",
    "    Args:\n",
    "        gt_images (ndarray): ground truth images read by the PyTorch library\n",
    "        lr_images (ndarray): low resolution images read by the PyTorch library\n",
    "        p (optional, float): flip probability. Default: 0.5\n",
    "\n",
    "    Returns:\n",
    "        gt_images (ndarray or Tensor or): flipped ground truth images\n",
    "        lr_images (ndarray or Tensor or): flipped low-resolution images\n",
    "\n",
    "    \"\"\"\n",
    "    # Randomly generate flip probability\n",
    "    flip_prob = random.random()\n",
    "\n",
    "    if not isinstance(gt_images, list):\n",
    "        gt_images = [gt_images]\n",
    "    if not isinstance(lr_images, list):\n",
    "        lr_images = [lr_images]\n",
    "\n",
    "    # detect input image type\n",
    "    input_type = \"Tensor\" if torch.is_tensor(lr_images[0]) else \"Numpy\"\n",
    "\n",
    "    if flip_prob > p:\n",
    "        if input_type == \"Tensor\":\n",
    "            lr_images = [F_vision.vflip(lr_image) for lr_image in lr_images]\n",
    "            gt_images = [F_vision.vflip(gt_image) for gt_image in gt_images]\n",
    "        else:\n",
    "            lr_images = [cv2.flip(lr_image, 0) for lr_image in lr_images]\n",
    "            gt_images = [cv2.flip(gt_image, 0) for gt_image in gt_images]\n",
    "\n",
    "    # When the input has only one image\n",
    "    if len(gt_images) == 1:\n",
    "        gt_images = gt_images[0]\n",
    "    if len(lr_images) == 1:\n",
    "        lr_images = lr_images[0]\n",
    "\n",
    "    return gt_images, lr_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import distributed as dist\n",
    "from enum import Enum\n",
    "\n",
    "class Summary(Enum):\n",
    "    NONE = 0\n",
    "    AVERAGE = 1\n",
    "    SUM = 2\n",
    "    COUNT = 3\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self, name, fmt=\":f\", summary_type=Summary.AVERAGE):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.summary_type = summary_type\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def all_reduce(self):\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        total = torch.tensor([self.sum, self.count], dtype=torch.float32, device=device)\n",
    "        dist.all_reduce(total, dist.ReduceOp.SUM, async_op=False)\n",
    "        self.sum, self.count = total.tolist()\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "    def summary(self):\n",
    "        if self.summary_type is Summary.NONE:\n",
    "            fmtstr = \"\"\n",
    "        elif self.summary_type is Summary.AVERAGE:\n",
    "            fmtstr = \"{name} {avg:.4f}\"\n",
    "        elif self.summary_type is Summary.SUM:\n",
    "            fmtstr = \"{name} {sum:.4f}\"\n",
    "        elif self.summary_type is Summary.COUNT:\n",
    "            fmtstr = \"{name} {count:.4f}\"\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid summary type {self.summary_type}\")\n",
    "\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print(\"\\t\".join(entries))\n",
    "\n",
    "    def display_summary(self):\n",
    "        entries = [\" *\"]\n",
    "        entries += [meter.summary() for meter in self.meters]\n",
    "        print(\" \".join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
    "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test(\n",
    "        g_model: nn.Module,\n",
    "        test_data_dataloader : DataLoader,\n",
    "        psnr_model: nn.Module,\n",
    "        ssim_model: nn.Module,\n",
    "        device: torch.device,\n",
    "        config: Any,\n",
    ") -> List[float]:\n",
    "    save_image = False\n",
    "    save_image_dir = \"\"\n",
    "\n",
    "    \n",
    "    save_image = True\n",
    "    save_image_dir = \"models\"\n",
    "\n",
    "    # Calculate the number of iterations per epoch\n",
    "    batches = len(test_data_dataloader)\n",
    "    # Interval printing\n",
    "    if batches > 100:\n",
    "        print_freq = 100\n",
    "    else:\n",
    "        print_freq = batches\n",
    "    # The information printed by the progress bar\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\", Summary.NONE)\n",
    "    psnres = AverageMeter(\"PSNR\", \":4.2f\", Summary.AVERAGE)\n",
    "    ssimes = AverageMeter(\"SSIM\", \":4.4f\", Summary.AVERAGE)\n",
    "    progress = ProgressMeter(len(test_data_dataloader),\n",
    "                             [batch_time, psnres, ssimes],\n",
    "                             prefix=f\"Test: \")\n",
    "\n",
    "    # set the model as validation model\n",
    "    g_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize data batches\n",
    "        batch_index = 0\n",
    "        \n",
    "\n",
    "        for lr, gt in tqdm(test_data_dataloader):\n",
    "            # Load batches of data\n",
    "            gt = gt.to(device, non_blocking=True)\n",
    "            lr = lr.to(device, non_blocking=True)\n",
    "\n",
    "            # Reasoning\n",
    "            sr = g_model(lr)\n",
    "\n",
    "            # Calculate the image sharpness evaluation index\n",
    "            psnr = psnr_model(sr, gt)\n",
    "            ssim = ssim_model(sr, gt)\n",
    "\n",
    "            # record current metrics\n",
    "            psnres.update(psnr.item(), sr.size(0))\n",
    "            ssimes.update(ssim.item(), ssim.size(0))\n",
    "\n",
    "            # Record the total time to verify a batch\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # Output a verification log information\n",
    "            if batch_index % print_freq == 0:\n",
    "                progress.display(batch_index)\n",
    "\n",
    "            # Add 1 to the number of data batches\n",
    "            batch_index += 1\n",
    "\n",
    "    # Print the performance index of the model at the current Epoch\n",
    "    progress.display_summary()\n",
    "\n",
    "    return psnres.avg, ssimes.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_tuple(dim: int):\n",
    "    \"\"\"Convert the input to a tuple\n",
    "\n",
    "    Args:\n",
    "        dim (int): the dimension of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def parse(x):\n",
    "        if isinstance(x, collections.abc.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, dim))\n",
    "\n",
    "    return parse\n",
    "\n",
    "\n",
    "def _fspecial_gaussian_torch(\n",
    "        window_size: int,\n",
    "        sigma: float,\n",
    "        channels: int = 3,\n",
    "        filter_type: int = 0,\n",
    ") -> Tensor:\n",
    "    \"\"\"PyTorch implements the fspecial_gaussian() function in MATLAB\n",
    "\n",
    "    Args:\n",
    "        window_size (int): Gaussian filter size\n",
    "        sigma (float): sigma parameter in Gaussian filter\n",
    "        channels (int): number of image channels, default: ``3``\n",
    "        filter_type (int): filter type, 0: Gaussian filter, 1: mean filter, default: ``0``\n",
    "\n",
    "    Returns:\n",
    "        gaussian_kernel_window (Tensor): Gaussian filter\n",
    "    \"\"\"\n",
    "\n",
    "    # Gaussian filter processing\n",
    "    if filter_type == 0:\n",
    "        shape = _to_tuple(2)(window_size)\n",
    "        m, n = [(ss - 1.) / 2. for ss in shape]\n",
    "        y, x = np.ogrid[-m:m + 1, -n:n + 1]\n",
    "        g = np.exp(-(x * x + y * y) / (2. * sigma * sigma))\n",
    "        g[g < np.finfo(g.dtype).eps * g.max()] = 0\n",
    "        sum_height = g.sum()\n",
    "\n",
    "        if sum_height != 0:\n",
    "            g /= sum_height\n",
    "\n",
    "        g = torch.from_numpy(g).float().repeat(channels, 1, 1, 1)\n",
    "\n",
    "        return g\n",
    "    # mean filter processing\n",
    "    elif filter_type == 1:\n",
    "        raise NotImplementedError(f\"Only support `gaussian filter`, got {filter_type}\")\n",
    "\n",
    "\n",
    "\n",
    "class PSNR(nn.Module):\n",
    "    \"\"\"PyTorch implements PSNR (Peak Signal-to-Noise Ratio, peak signal-to-noise ratio) function\"\"\"\n",
    "\n",
    "    def __init__(self, crop_border: int = 0, only_test_y_channel: bool = True, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            crop_border (int, optional): how many pixels to crop border. Default: 0\n",
    "            only_test_y_channel (bool, optional): Whether to test only the Y channel of the image. Default: ``True``\n",
    "\n",
    "        Returns:\n",
    "            psnr_metrics (Tensor): PSNR metrics\n",
    "        \"\"\"\n",
    "        super(PSNR, self).__init__()\n",
    "        self.crop_border = crop_border\n",
    "        self.only_test_y_channel = only_test_y_channel\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def forward(self, raw_tensor: Tensor, dst_tensor: Tensor) -> Tensor:\n",
    "        # Check if two tensor scales are similar\n",
    "        _check_tensor_shape(raw_tensor, dst_tensor)\n",
    "\n",
    "        # crop pixel boundaries\n",
    "        if self.crop_border > 0:\n",
    "            raw_tensor = raw_tensor[..., self.crop_border:-self.crop_border, self.crop_border:-self.crop_border]\n",
    "            dst_tensor = dst_tensor[..., self.crop_border:-self.crop_border, self.crop_border:-self.crop_border]\n",
    "\n",
    "        psnr_metrics = _psnr_torch(raw_tensor, dst_tensor, self.only_test_y_channel, **self.kwargs)\n",
    "\n",
    "        return psnr_metrics\n",
    "\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    \"\"\"PyTorch implements SSIM (Structural Similarity) function\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            window_size: int = 11,\n",
    "            gaussian_sigma: float = 1.5,\n",
    "            channels: int = 3,\n",
    "            downsampling: bool = False,\n",
    "            get_ssim_map: bool = False,\n",
    "            get_cs_map: bool = False,\n",
    "            get_weight: bool = False,\n",
    "            crop_border: int = 0,\n",
    "            only_test_y_channel: bool = True,\n",
    "            **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            window_size (int): Gaussian filter size, must be an odd number, default: ``11``\n",
    "            gaussian_sigma (float): sigma parameter in Gaussian filter, default: ``1.5``\n",
    "            channels (int): number of image channels, default: ``3``\n",
    "            downsampling (bool): Whether to perform downsampling, default: ``False``\n",
    "            get_ssim_map (bool): Whether to return SSIM image, default: ``False``\n",
    "            get_cs_map (bool): whether to return CS image, default: ``False``\n",
    "            get_weight (bool): whether to return the weight image, default: ``False``\n",
    "            crop_border (int, optional): how many pixels to crop border. Default: 0\n",
    "            only_test_y_channel (bool, optional): Whether to test only the Y channel of the image. Default: ``True``\n",
    "\n",
    "        Returns:\n",
    "            ssim_metrics (Tensor): SSIM metrics\n",
    "\n",
    "        \"\"\"\n",
    "        super(SSIM, self).__init__()\n",
    "        if only_test_y_channel and channels != 1:\n",
    "            channels = 1\n",
    "        self.gaussian_kernel_window = _fspecial_gaussian_torch(window_size, gaussian_sigma, channels)\n",
    "        self.downsampling = downsampling\n",
    "        self.get_ssim_map = get_ssim_map\n",
    "        self.get_cs_map = get_cs_map\n",
    "        self.get_weight = get_weight\n",
    "        self.crop_border = crop_border\n",
    "        self.only_test_y_channel = only_test_y_channel\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def forward(self, raw_tensor: Tensor, dst_tensor: Tensor) -> Tensor:\n",
    "        # Check if two tensor scales are similar\n",
    "        _check_tensor_shape(raw_tensor, dst_tensor)\n",
    "\n",
    "        # crop pixel boundaries\n",
    "        if self.crop_border > 0:\n",
    "            raw_tensor = raw_tensor[..., self.crop_border:-self.crop_border, self.crop_border:-self.crop_border]\n",
    "            dst_tensor = dst_tensor[..., self.crop_border:-self.crop_border, self.crop_border:-self.crop_border]\n",
    "\n",
    "        ssim_metrics = _ssim_torch(raw_tensor,\n",
    "                                   dst_tensor,\n",
    "                                   self.gaussian_kernel_window,\n",
    "                                   self.downsampling,\n",
    "                                   self.get_ssim_map,\n",
    "                                   self.get_cs_map,\n",
    "                                   self.get_weight,\n",
    "                                   self.only_test_y_channel,\n",
    "                                   **self.kwargs)\n",
    "\n",
    "        return ssim_metrics\n",
    "\n",
    "def build_iqa_model(\n",
    "        crop_border: int,\n",
    "        only_test_y_channel: bool,\n",
    "        device: torch.device,\n",
    ") -> tuple[Any, Any]:\n",
    "    psnr_model = PSNR(crop_border=crop_border, only_test_y_channel=only_test_y_channel, data_range=1.0)\n",
    "    ssim_model = SSIM(crop_border=crop_border, only_test_y_channel=only_test_y_channel, data_range=255.0)\n",
    "\n",
    "    psnr_model = psnr_model.to(device)\n",
    "    ssim_model = ssim_model.to(device)\n",
    "\n",
    "    return psnr_model, ssim_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from typing import Any, Optional\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from torch import nn, optim\n",
    "from torch.backends import cudnn\n",
    "from torch.cuda import amp\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import collections.abc\n",
    "import math\n",
    "import typing\n",
    "import warnings\n",
    "from itertools import repeat\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F_torch\n",
    "\n",
    "\n",
    "def build_model(device: torch.device) -> [nn.Module , Optional[nn.Module | Any] , nn.Module]:\n",
    "\n",
    "    g_model = rrdbnet_x2(in_channels = 1,out_channels = 1)\n",
    "    d_model = discriminator_for_vgg(in_channels=1, out_channels=1)\n",
    "    \n",
    "    g_model = g_model.to(device)\n",
    "    d_model = d_model.to(device)\n",
    "\n",
    "    if False:\n",
    "        # Generate an exponential average model based on a generator to stabilize model training\n",
    "        ema_decay =  0.999\n",
    "        ema_avg_fn = lambda averaged_model_parameter, model_parameter, num_averaged: \\\n",
    "            (1 - ema_decay) * averaged_model_parameter + ema_decay * model_parameter\n",
    "        ema_g_model = AveragedModel(g_model, device=device, avg_fn=ema_avg_fn)\n",
    "    else:\n",
    "        ema_g_model = None\n",
    "\n",
    "    # compile model\n",
    "    if CONFIG.COMPILE:\n",
    "        g_model = torch.compile(g_model)\n",
    "    if CONFIG.COMPILE:\n",
    "        d_model = torch.compile(d_model)\n",
    "    if CONFIG.COMPILE and ema_g_model is not None:\n",
    "        ema_g_model = torch.compile(ema_g_model)\n",
    "\n",
    "    return g_model, ema_g_model, d_model\n",
    "\n",
    "\n",
    "def define_loss(device: torch.device):\n",
    "    \n",
    "    pixel_criterion = nn.L1Loss()\n",
    "    \n",
    "    \"\"\"\n",
    "    feature_criterion = ContentLoss(\n",
    "            net_cfg_name=\"vgg19\",\n",
    "            batch_norm=False,\n",
    "            num_classes=1000,\n",
    "            model_weights_path=\"\",\n",
    "            feature_nodes=[\"features.34\"],\n",
    "            feature_normalize_mean=[0.],\n",
    "            feature_normalize_std=[1.],\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    adversarial_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    pixel_criterion = pixel_criterion.to(device)\n",
    "    #feature_criterion = feature_criterion.to(device)\n",
    "    adversarial_criterion = adversarial_criterion.to(device)\n",
    "\n",
    "    return pixel_criterion, None, adversarial_criterion\n",
    "\n",
    "\n",
    "def define_optimizer(g_model: nn.Module, d_model: nn.Module) -> [optim.Adam, optim.Adam]:\n",
    "    g_optimizer = optim.Adam(g_model.parameters(),\n",
    "                                 lr=0.0001,\n",
    "                                 betas= [0.9, 0.999],\n",
    "                                 eps=0.0001,\n",
    "                                 weight_decay=0.0)\n",
    "    d_optimizer = optim.Adam(d_model.parameters(),\n",
    "                                 lr=0.0001,\n",
    "                                 betas= [0.9, 0.999],\n",
    "                                 eps=0.0001,\n",
    "                                 weight_decay=0.0)\n",
    "\n",
    "    return g_optimizer, d_optimizer\n",
    "\n",
    "\n",
    "def define_scheduler(g_optimizer: optim.Adam, d_optimizer: optim.Adam) -> [lr_scheduler.MultiStepLR, lr_scheduler.MultiStepLR]:\n",
    "    \n",
    "    g_scheduler = lr_scheduler.MultiStepLR(g_optimizer,\n",
    "                                               milestones=[ 16, 32, 64, 104 ],\n",
    "                                               gamma=0.5)\n",
    "    d_scheduler = lr_scheduler.MultiStepLR(d_optimizer,\n",
    "                                               milestones=[ 16, 32, 64, 104 ],\n",
    "                                               gamma=0.5)\n",
    "    \n",
    "    return g_scheduler, d_scheduler\n",
    "\n",
    "def train(\n",
    "        g_model: nn.Module,\n",
    "        ema_g_model: nn.Module,\n",
    "        d_model: nn.Module,\n",
    "        train_data: DataLoader,\n",
    "        pixel_criterion: nn.L1Loss,\n",
    "        content_criterion: ContentLoss,\n",
    "        adversarial_criterion: nn.BCEWithLogitsLoss,\n",
    "        g_optimizer: optim.Adam,\n",
    "        d_optimizer: optim.Adam,\n",
    "        epoch: int,\n",
    "        scaler: amp.GradScaler,\n",
    "        device: torch.device,\n",
    ") -> None:\n",
    "    # Calculate how many batches of data there are under a dataset iterator\n",
    "    batches = len(train_data)\n",
    "\n",
    "    # Set the model to training mode\n",
    "    g_model.train()\n",
    "    d_model.train()\n",
    "\n",
    "    # Define loss function weights\n",
    "    pixel_weight = torch.Tensor([0.01]).to(device)\n",
    "    content_weight = torch.Tensor([1.0]).to(device)\n",
    "    adversarial_weight = torch.Tensor([0.005]).to(device)\n",
    "\n",
    "    # Initialize data batches\n",
    "    batch_index = 0\n",
    "    # Record the start time of training a batch\n",
    "    end = time.time()\n",
    "    \n",
    "    # load the first batch of data\n",
    "    # batch_data = train_data_prefetcher.next()\n",
    "\n",
    "    # Used for discriminator binary classification output, the input sample comes from the data set (real sample) is marked as 1, and the input sample comes from the generator (generated sample) is marked as 0\n",
    "    batch_size = CONFIG.BATCH_SIZE\n",
    "    \n",
    "    real_label = torch.full([batch_size, 1], 1.0, dtype=torch.float, device=device)\n",
    "    fake_label = torch.full([batch_size, 1], 0.0, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "    pbar = tqdm(train_data)\n",
    "\n",
    "    print(\"Starting Epoch\")\n",
    "    for gt, lr in pbar:\n",
    "        # Load batches of data\n",
    "        gt = gt.to(device, non_blocking=True)\n",
    "        lr = lr.to(device, non_blocking=True)\n",
    "        print(\"GET\")\n",
    "\n",
    "        # image data augmentation\n",
    "        #gt, lr = random_crop_torch(gt,\n",
    "        #                           lr,\n",
    "        #                           config[\"TRAIN\"][\"DATASET\"][\"GT_IMAGE_SIZE\"],\n",
    "        #                           config[\"SCALE\"])\n",
    "        #gt, lr = random_rotate_torch(gt, lr, config[\"SCALE\"], [0, 90, 180, 270])\n",
    "        #gt, lr = random_vertically_flip_torch(gt, lr)\n",
    "        #gt, lr = random_horizontally_flip_torch(gt, lr)\n",
    "\n",
    "        # Record the time to load a batch of data\n",
    "        # data_time.update(time.time() - end)\n",
    "\n",
    "        # start training the generator model\n",
    "        # Disable discriminator backpropagation during generator training\n",
    "        for d_parameters in d_model.parameters():\n",
    "            d_parameters.requires_grad = False\n",
    "        print(\"parameter freeze\")\n",
    "        \n",
    "        # Initialize the generator model gradient\n",
    "        g_model.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Calculate the perceptual loss of the generator, mainly including pixel loss, feature loss and confrontation loss\n",
    "        sr = g_model(lr)\n",
    "        print(\"get the val\")\n",
    " \n",
    "        \n",
    "        # Output discriminator to discriminate object probability\n",
    "        gt_output = d_model(gt.detach().clone())\n",
    "        print(\"discriminator GT\")\n",
    "\n",
    "        sr_output = d_model(sr)\n",
    "        print(\"discriminator SR\")\n",
    "\n",
    "        pixel_loss = pixel_criterion(sr, gt)\n",
    "        print(\"pixel crit\")\n",
    "\n",
    "        #content_loss = content_criterion(sr, gt)\n",
    "        #print(\"content crit\")\n",
    "\n",
    "\n",
    "        d_loss_gt = adversarial_criterion(gt_output - torch.mean(sr_output), fake_label) * 0.5\n",
    "        d_loss_sr = adversarial_criterion(sr_output - torch.mean(gt_output), real_label) * 0.5\n",
    "        adversarial_loss = d_loss_gt + d_loss_sr\n",
    "        pixel_loss = torch.sum(torch.mul(pixel_weight, pixel_loss))\n",
    "        #content_loss = torch.sum(torch.mul(content_weight, content_loss))\n",
    "        adversarial_loss = torch.sum(torch.mul(adversarial_weight, adversarial_loss))\n",
    "        print(\"loss calculated\")\n",
    "        \n",
    "        # Compute generator total loss\n",
    "        g_loss = pixel_loss + adversarial_loss\n",
    "        print(\"loss calculated\")\n",
    "        \n",
    "        # Backpropagation generator loss on generated samples\n",
    "        scaler.scale(g_loss).backward()\n",
    "        # update generator model weights\n",
    "        scaler.step(g_optimizer)\n",
    "        scaler.update()\n",
    "        # end training generator model\n",
    "\n",
    "        # start training the discriminator model\n",
    "        # During discriminator model training, enable discriminator model backpropagation\n",
    "        for d_parameters in d_model.parameters():\n",
    "            d_parameters.requires_grad = True\n",
    "\n",
    "        # Initialize the discriminator model gradient\n",
    "        d_model.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Calculate the classification score of the discriminator model for real samples\n",
    "        \n",
    "        gt_output = d_model(gt)\n",
    "        sr_output = d_model(sr.detach().clone())\n",
    "        d_loss_gt = adversarial_criterion(gt_output - torch.mean(sr_output), real_label) * 0.5\n",
    "        \n",
    "        # Call the gradient scaling function in the mixed precision API to\n",
    "        # back-propagate the gradient information of the fake samples\n",
    "        scaler.scale(d_loss_gt).backward(retain_graph=True)\n",
    "\n",
    "        # Calculate the classification score of the discriminator model for fake samples\n",
    "        \n",
    "        sr_output = d_model(sr.detach().clone())\n",
    "        d_loss_sr = adversarial_criterion(sr_output - torch.mean(gt_output), fake_label) * 0.5\n",
    "        # Call the gradient scaling function in the mixed precision API to\n",
    "        # back-propagate the gradient information of the fake samples\n",
    "        scaler.scale(d_loss_sr).backward()\n",
    "\n",
    "        # Calculate the total discriminator loss value\n",
    "        d_loss = d_loss_gt + d_loss_sr\n",
    "\n",
    "        # Update discriminator model weights\n",
    "        scaler.step(d_optimizer)\n",
    "        scaler.update()\n",
    "        # end training discriminator model\n",
    "\n",
    "        #if config[\"MODEL\"][\"EMA\"][\"ENABLE\"]:\n",
    "        #    # update exponentially averaged model weights\n",
    "        #    ema_g_model.update_parameters(g_model)\n",
    "\n",
    "        # record the loss value\n",
    "\n",
    "        # Record the total time of training a batch\n",
    "        #batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # Output training log information once\n",
    "        if batch_index % CONFIG.PRINT_FREQ == 0:\n",
    "            # write training log\n",
    "            iters = batch_index + epoch * batches\n",
    "            \n",
    "            print(\"Train/D_Loss\", d_loss.item(), iters)\n",
    "            print(\"Train/D(GT)_Loss\", d_loss_gt.item(), iters)\n",
    "            print(\"Train/D(SR)_Loss\", d_loss_sr.item(), iters)\n",
    "            print(\"Train/G_Loss\", g_loss.item(), iters)\n",
    "            print(\"Train/Pixel_Loss\", pixel_loss.item(), iters)\n",
    "            print(\"Train/Adversarial_Loss\", adversarial_loss.item(), iters)\n",
    "            print(\"Train/D(GT)_Probability\", torch.sigmoid_(torch.mean(gt_output.detach())).item(), iters)\n",
    "            print(\"Train/D(SR)_Probability\", torch.sigmoid_(torch.mean(sr_output.detach())).item(), iters)\n",
    "            pbar.set_description()\n",
    "\n",
    "\n",
    "        # After training a batch of data, add 1 to the number of data batches to ensure that the terminal prints data normally\n",
    "        batch_index += 1\n",
    "\n",
    "\n",
    "# Because the size of the input image is fixed, the fixed CUDNN convolution method can greatly increase the running speed\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Initialize the mixed precision method\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "# Default to start training from scratch\n",
    "start_epoch = 0\n",
    "\n",
    "# Initialize the image clarity evaluation index\n",
    "best_psnr = 0.0\n",
    "best_ssim = 0.0\n",
    "\n",
    "# Define the running device number\n",
    "device = CONFIG.DEVICE\n",
    "\n",
    "# Define the basic functions needed to start training\n",
    "train_data_prefetcher, paired_test_data_prefetcher = train_loader, val_loader\n",
    "g_model, ema_g_model, d_model = build_model(device)\n",
    "    \n",
    "pixel_criterion, feature_criterion, adversarial_criterion = define_loss(device)\n",
    "g_optimizer, d_optimizer = define_optimizer(g_model, d_model)\n",
    "g_scheduler, d_scheduler = define_scheduler(g_optimizer, d_optimizer)\n",
    "\n",
    "\n",
    "# Initialize the image clarity evaluation method\n",
    "psnr_model, ssim_model = build_iqa_model(CONFIG.SCALE, False, device)\n",
    "\n",
    "# Create the folder where the model weights are saved\n",
    "results_dir = \"models\"\n",
    "\n",
    "for epoch in range(start_epoch, CONFIG.EPOCHS):\n",
    "    train(g_model,\n",
    "          ema_g_model,\n",
    "          d_model,\n",
    "          train_data_prefetcher,\n",
    "          pixel_criterion,\n",
    "          feature_criterion,\n",
    "          adversarial_criterion,\n",
    "          g_optimizer,\n",
    "          d_optimizer,\n",
    "          epoch,\n",
    "          scaler,\n",
    "          device)\n",
    "\n",
    "    # Update LR\n",
    "    g_scheduler.step()\n",
    "    d_scheduler.step()\n",
    "\n",
    "    psnr, ssim = test(g_model,\n",
    "                        paired_test_data_prefetcher,\n",
    "                        psnr_model,\n",
    "                        ssim_model,\n",
    "                        device)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    # Automatically save model weights\n",
    "    is_best = psnr > best_psnr and ssim > best_ssim\n",
    "    is_last = (epoch + 1) == CONFIG.EPOCHS\n",
    "\n",
    "    best_psnr = max(psnr, best_psnr)\n",
    "    best_ssim = max(ssim, best_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "g_model, ema_g_model, d_model = build_model(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
